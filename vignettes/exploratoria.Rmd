---
title: "Analise exploratoria inicial"
author: "Julio Trecenti"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Analise exploratoria dos dados}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r}
# dados e api
library(statjobs)

# hadley
library(dplyr)
library(tidyr)
library(stringr)
library(ggplot2)
library(lubridate)

# text mining
library(tm)
library(textcat)
library(RWeka)
library(wordcloud)
```

## Analisando evolucao da Doris e do Guilherme no grupo

```{r fig.width=9, fig.height=6}
posts %>%
  filter(from_name %in% c('Doris S M Fontes', 
                          'Guilherme Carrara Neto')) %>%
  mutate(time = ymd_hms(created_time)) %>%
  arrange(time) %>%
  group_by(from_name) %>%
  mutate(um = 1, acu = cumsum(um)) %>%
  ungroup %>%
  ggplot(aes(x = time, y = acu, colour = from_name)) +
  geom_line() +
  theme_bw()
```

## Tratando os textos e analisando com text mining usual

Primeiro, vamos dar uma limpada nos textos. Consideramos só os textos em
portugues, de inicio!

- ignore case
- remove punctuation
- remove stop words
- remove accents
- remove numbers
- remove links and emails

```{r}
prof <- TC_byte_profiles[names(TC_byte_profiles) %in% c("english", "portuguese")]
posts_messages <- posts %>%
  filter(!is.na(message), type %in% c('status', 'link')) %>%
  mutate(lingua = textcat(message, p = prof)) %>%
  filter(lingua == 'portuguese')

# rm_accent SO FUNCIONA EM LINUX!!!!!!!!!!!!!!!!!!!!
rm_accent <- function(x) gsub("`|\\'", "", iconv(x, to = "ASCII//TRANSLIT"))
tira_barras <- function(x) gsub('/+|(\\:)+', ' ', x)
tira_emails <- function(x) gsub('[a-zA-Z]+@[a-zA-Z\\-]+\\.[a-zA-Z]+', '', x)
tira_links <- function(x) gsub('http[^ ]+', ' ', x)
# tira_s <- function(x) {gsub('spss', 'spsss', x); gsub('s ', ' ', x)}

banned_words <- c('vaga[^ ]*', 'todo', 'toda', 'trabalh[^ ]*', 
                  'paulo', 'acompanha[^ ]*', 'principal[^ ]*',
                  'profission[^ ]*', 'estatistic[^ ]*',
                  'expiracao', 'janeiro', 'fevereiro', 'marco',
                  'abril', 'maio', 'junho', 'julho', 'agosto',
                  'setembro', 'outubro', 'novembro', 'dezembro',
                  'senior', 'pleno', 'dado', 'data', 'local',
                  'codigo', 'nivel', 'hierarquico', 'rio', 'junior',
                  'estagio', 'quantidade', ' [a-z] ')
banned_words <- paste(banned_words, collapse = '|')
tira_banned <- function(x) gsub(banned_words, ' ', x)

d_tm_raw <- VCorpus(VectorSource(posts_messages$message))
d_tm <- d_tm_raw %>%
  tm_map(stripWhitespace) %>%
  tm_map(content_transformer(tolower)) %>%
  tm_map(content_transformer(tira_emails)) %>%
  tm_map(content_transformer(tira_links)) %>%
  tm_map(content_transformer(tira_barras)) %>%
  tm_map(removeWords, stopwords('pt-br')) %>%
  tm_map(removePunctuation, preserve_intra_word_dashes = TRUE) %>%
  tm_map(content_transformer(rm_accent)) %>%
  tm_map(content_transformer(tira_links)) %>%
  tm_map(removeWords, unique(rm_accent(stopwords('pt-br')))) %>%
  tm_map(content_transformer(tira_banned)) %>%
  # tm_map(content_transformer(tira_s)) %>%
  tm_map(removeNumbers) %>%
  tm_map(stripWhitespace)
```

### Term document matrix, usando 1-gram e tfidf weighting, sem stemming

```{r}
ctrl <- list(weighting = function(x) weightTfIdf(x, normalize = FALSE))
dtm <- DocumentTermMatrix(d_tm, control = ctrl)
dtm <- removeSparseTerms(dtm, 0.95)

# como padrao dtm é uma matriz esparsa, boa pra guardar dados mas ruim pra explorar
# por sorte a base de dados é pequena e da pra transformar em data.frame
d_dtm <- data.frame(as.matrix(dtm)) %>%
  add_rownames() %>%
  tbl_df()

# pegando as 10 palavras "mais informativas" de cada documento
palavras <- d_dtm %>%
  mutate(rowname = as.numeric(rowname)) %>%
  gather(key, val, -rowname) %>%
  arrange(desc(val)) %>%
  group_by(rowname) %>%
  slice(1:10) %>%
  ungroup %>%
  filter(val > 0)

wordcloud(palavras$key, 
          max.words = 100, 
          colors = brewer.pal(8, 'Dark2'))
```

### Term document matrix, usando 2-gram e tfidf weighting, sem stemming

Nao rodou :(

```{r}
# BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
# ctrl <- list(tokenize = BigramTokenizer)
# dtm <- DocumentTermMatrix(d_tm, control = ctrl)
# dtm <- removeSparseTerms(dtm, 0.95)
# 
# # como padrao dtm é uma matriz esparsa, boa pra guardar dados mas ruim pra explorar
# # por sorte a base de dados é pequena e da pra transformar em data.frame
# d_dtm <- data.frame(as.matrix(dtm)) %>%
#   add_rownames() %>%
#   tbl_df()
# 
# # pegando as 10 palavras "mais informativas" de cada documento
# palavras <- d_dtm %>%
#   mutate(rowname = as.numeric(rowname)) %>%
#   gather(key, val, -rowname) %>%
#   arrange(desc(val)) %>%
#   group_by(rowname) %>%
#   slice(1:10) %>%
#   ungroup %>%
#   filter(val > 0)
# 
# library(wordcloud)
# wordcloud(palavras$key, 
#           max.words = 100, 
#           colors = brewer.pal(8, 'Dark2'))
```

## Analisando as mensagens

Tipos de posts

```{r}
posts %>%
  count(type, sort = TRUE) %>%
  mutate(` %` = round(n/sum(n) * 100, 2))
```

Note que são dois tipos de posts principais, status e links. Vamos analisar
separadamente.

```{r}
status <- posts %>% 
  filter(type %in% 'status')

link <- posts %>% 
  filter(type %in% 'link')
```

Em relação aos status, podemos notar que existe uma grande quantidade de
vagas do site www:.vagas.com.br

```{r}
status %>%
  mutate(vagas = str_detect(message, 'www\\.vagas\\.com\\.br')) %>%
  count(vagas, sort = TRUE)
```

Geralmente, os posts desse site contém um padrão conhecido.

```{r}
aux <- status %>%
  mutate(vagas = str_detect(message, 'www\\.vagas\\.com\\.br')) %>%
  filter(vagas) %>%
  slice(1) %>% 
  select(from_name, message) %>% 
  sapply(function(x) {cat(x); cat('\n\n@@@@@@@\n\n')})
```

Algumas informações úteis que podem ser extraídas:

- Nível hierárquico
- Nome da vaga (analista, estatístico, etc)
- Atividades
- Requisitos

```{r}

antes <- c('([^a-zA-Z]|^)formacao\\:?\n?( e pre\\-requisitos\\:)?',
           'academica\\:?\n?',
           '([^a-zA-Z]|^)cursar',
           '([^a-zA-Z]|^)curso\\:?',
           'superior',
           'graduacao')
re <- sprintf('(%s)([^\n]+)', paste(antes, collapse = '|'))
vagas <- status %>%
  mutate(vagas = str_detect(message, 'www\\.vagas\\.com\\.br')) %>%
  filter(vagas) %>%
  mutate(nivel = str_match(message, 'Nível hierárquico\\:([^\n]+)\n')[, 2]) %>%
  mutate(local = str_match(message, 'Local\\:([^\n]+)\n')[, 2]) %>%
  mutate(req = str_match(message, regex('req[^:]+\\:([^|]+)',
                                        ignore_case = TRUE))[, 2]) %>%
  mutate(form = str_match(tolower(tjsp::rm_accent(message)), re)[, 7])
```

Em relação ao nível hierárquico, temos o seguinte resultado:

```{r}
vagas %>%
  count(nivel, sort = TRUE) %>%
  mutate(` %` = round(n/sum(n) * 100, 2)) %>%
  knitr::kable()
```

Em relação ao local, temos:

```{r}
vagas %>%
  count(local, sort = TRUE) %>%
  mutate(` %` = round(n/sum(n) * 100, 2)) %>%
  head(10) %>%
  knitr::kable()
```

O grafico abaixo mostra quais os termos mais frequentes quando se fala em formação.

```{r fig.width=9, fig.height=6}
# aux <- vagas %>%
#   sample_n(1) %>% 
#   select(from_name, message, form) %>% 
#   sapply(function(x) {cat(x); cat('\n\n@@@@@@@\n\n')})

cursos <- vagas %>%
  filter(!is.na(form)) %>%
  mutate(matematica = ifelse(str_detect(form, 'matematica'), 'matematica', NA)) %>%
  mutate(contabeis = ifelse(str_detect(form, 'ciencias cont|contab'), 
                            'ciencias contabeis', NA)) %>%
  mutate(engenharia = ifelse(str_detect(form, 'engenharia'), 'engenharia', NA)) %>%
  mutate(economia = ifelse(str_detect(form, 'economia'), 'economia', NA)) %>%
  mutate(financas = ifelse(str_detect(form, 'financas'), 'financas', NA)) %>%
  mutate(atuaria = ifelse(str_detect(form, 'atuari'), 'atuaria', NA)) %>%
  mutate(administracao = ifelse(str_detect(form, 'admin'), 'administracao', NA)) %>%
  mutate(marketing = ifelse(str_detect(form, 'marketing'), 'marketing', NA)) %>%
  mutate(informatica = ifelse(str_detect(form, 'inform|compu'), 'informatica', NA)) %>%
  mutate(estatistica = ifelse(str_detect(form, 'estat'), 'estatistica', NA))
  
cursos %>%
  summarise_each(funs(sum(!is.na(.))), matematica:estatistica) %>%
  gather() %>%
  ggplot(aes(x = reorder(key, X = value), y = value, fill = key)) +
  geom_bar(stat = 'identity') +
  guides(fill = FALSE) +
  theme_bw() +
  xlab('formacao') +
  ylab('quantidade') +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 12))
```

O grafico abaixo mostra quais os termos mais frequentes quando se fala em formação,
apenas quando aparece o nome "estatistica".

```{r fig.width=9, fig.height=6}
cursos %>%
  filter(!is.na(estatistica)) %>%
  summarise_each(funs(sum(!is.na(.))), matematica:estatistica) %>%
  gather() %>%
  mutate(value = value / value[key=='estatistica']) %>%
  filter(key != 'estatistica') %>%
  ggplot(aes(x = reorder(key, X = value), y = value, fill = key)) +
  geom_bar(stat = 'identity') +
  guides(fill = FALSE) +
  theme_bw() +
  scale_y_continuous(labels = scales::percent, breaks = 0:10/10) +
  xlab('formacao') +
  ylab('proporcao') +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 12))
```

TODO

- Pegar atividades
- Pegar requisitos
- Pegar desejáveis
